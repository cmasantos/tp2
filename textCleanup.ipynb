{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "FILE_BASE_PATH                              = \"./data/data/ende/\"\n",
    "DEFAULT_OUTPUT_DATASET_FILE_NAME            = \"dataset.txt\"\n",
    "CLEAR_HTML_TAG_REGULAR_EXPRESSION           = re.compile(\"<.*?>\")\n",
    "CLEAR_SPECIAL_HTML_CHARS_REGULAR_EXPRESSION = re.compile(\"(&)\\w+?;\")\n",
    "UKNOWN_CHARS_REGULAR_EXPRESSION             = re.compile(\"[^a-zA-Z0-9 \\t.,-ä)'$’\\\"\\%(#—“”!–öü+éá‘€*ó•\\−―ʼäëïöüçáéíóúñ✦„─]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readJsonFile(path): \n",
    "    file = open(path, encoding=\"utf-8\")\n",
    "\n",
    "    fileAsJson = json.load(file)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return fileAsJson\n",
    "\n",
    "def removeHtmlTagsFromString(originalString):\n",
    "    return re.sub(CLEAR_HTML_TAG_REGULAR_EXPRESSION, \"\", originalString)\n",
    "\n",
    "def removeHtmlSpecialCharsFromString(originalString):\n",
    "    return re.sub(CLEAR_SPECIAL_HTML_CHARS_REGULAR_EXPRESSION, \"\", originalString)\n",
    "\n",
    "def removeUnknowChars(originalString): \n",
    "\n",
    "    if(len(re.findall(UKNOWN_CHARS_REGULAR_EXPRESSION, originalString))):\n",
    "        return \"\"\n",
    "    \n",
    "    return originalString\n",
    "\n",
    "def clearString(originalString):\n",
    "\n",
    "    x = removeHtmlTagsFromString(originalString)\n",
    "    x = removeHtmlSpecialCharsFromString(x)\n",
    "    x = removeUnknowChars(x)\n",
    "    x = x.replace(\"\\t\", \" \") \n",
    "    x = x.replace(\"\\t\", \" \") \n",
    "    x = x.replace(\"à\", \"a\")\n",
    "    x = x.replace(\"’\", \"'\")\n",
    "    x = x.replace(\"–\", \"-\")\n",
    "    x = x.replace(\"“\", \"\\\"\")\n",
    "    x = x.replace(\"”\", \"\\\"\")\n",
    "    x = x.replace(\"‘\", \"'\")\n",
    "    x = x.lower()\n",
    "    return x\n",
    "\n",
    "def readJsonClean(jsonText):\n",
    "    return {key : clearString(value) for key, value in jsonText[\"text\"].items()}\n",
    "\n",
    "def createPairsArray(source, target):\n",
    "    return [f\"{value}\\t{target.get(key)}\\n\" for key, value in source.items()]\n",
    "    \n",
    "def createDatasetFile(source, target, destinationFileName=DEFAULT_OUTPUT_DATASET_FILE_NAME):\n",
    "    outputFile = open(destinationFileName, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    for key, value in source.items():\n",
    "        outputFile.write(f\"{value}\\t{target.get(key)}\\n\")  \n",
    "\n",
    "    outputFile.close()\n",
    "\n",
    "def createDatasetFileFromRows(rows, destinationFileName=DEFAULT_OUTPUT_DATASET_FILE_NAME):\n",
    "    outputFile = open(destinationFileName, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    for row in rows:\n",
    "        outputFile.write(row) \n",
    " \n",
    "\n",
    "    outputFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourFilePath   = FILE_BASE_PATH + \"ende_en_dev.json\"\n",
    "targetFilePath = FILE_BASE_PATH + \"ende_de_dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jsonSource = readJsonFile(sourFilePath)\n",
    "jsonTarget = readJsonFile(targetFilePath) \n",
    "\n",
    "jsonSourceClean = readJsonClean(jsonSource)\n",
    "jsonTargetClean = readJsonClean(jsonTarget)\n",
    "\n",
    "createDatasetFile(jsonSourceClean, jsonTargetClean, \"preprocessed_dataset_for_dev.txt\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourFilePath   = FILE_BASE_PATH + \"ende_en_train.json\"\n",
    "targetFilePath = FILE_BASE_PATH + \"ende_de_train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonSource = readJsonFile(sourFilePath)\n",
    "jsonTarget = readJsonFile(targetFilePath) \n",
    "\n",
    "jsonSourceClean = readJsonClean(jsonSource)\n",
    "jsonTargetClean = readJsonClean(jsonTarget)  \n",
    "\n",
    "createDatasetFile(jsonSourceClean, jsonTargetClean, \"preprocessed_dataset_for_train.txt\") \n",
    "pairsArray = createPairsArray(jsonSourceClean, jsonTargetClean) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourFilePath   = FILE_BASE_PATH + \"ende_en_train.json\"\n",
    "targetFilePath = FILE_BASE_PATH + \"ende_de_train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonSource = readJsonFile(sourFilePath)\n",
    "jsonTarget = readJsonFile(targetFilePath) \n",
    "\n",
    "jsonSourceClean = readJsonClean(jsonSource)\n",
    "jsonTargetClean = readJsonClean(jsonTarget)  \n",
    "\n",
    "createDatasetFile(jsonSourceClean, jsonTargetClean, \"preprocessed_dataset_for_train.txt\") \n",
    "pairsArray = createPairsArray(jsonSourceClean, jsonTargetClean) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error is between 50306 100611\n",
      "the error is between 75458 100611\n",
      "the error is between 88034 100611\n",
      "the error is between 88034 94322  \n",
      "the error is between 88034 91178  \n",
      "the error is between 88034 89606  \n",
      "the error is between 88034 88820  \n",
      "the error is between 88034 88427  \n",
      "the error is between 88230 88427\n",
      "the error is between 88328 88427\n",
      "the error is between 88328 88377  \n",
      "WARNING:tensorflow:5 out of the last 12 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x000001B81DF4AF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "the error is between 88352 88377\n",
      "WARNING:tensorflow:6 out of the last 13 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x000001B81DF4A670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "the error is between 88364 88377\n",
      "the error is between 88364 88370  \n",
      "the error is between 88364 88367  \n",
      "the error is between 88364 88365  \n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "#text_file = \"./train.txt\"\n",
    "#with open(text_file, encoding='utf-8') as f:\n",
    "#    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "\n",
    "lines = pairsArray\n",
    "\n",
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    english, port = line.split(\"\\t\")\n",
    "    port = \"[start] \" + port + \" [end]\"\n",
    "    text_pairs.append((english, port))\n",
    "\n",
    "train_english_texts = [pair[0] for pair in text_pairs] \n",
    "vocab_size = 15000    # O modelo apneas vai conhecer 15000 palavras\n",
    "sequence_length = 200  # cada frase vai ter 20 palavrasg\n",
    "\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "max = len(train_english_texts)\n",
    "a = 1\n",
    "b = max \n",
    "current = b // 2\n",
    "\n",
    "hasError = False\n",
    "\n",
    "while(( b- a) > 1):\n",
    "    half = a + (b - a) // 2\n",
    "    #print(\"\\ncurrent half: \",half)\n",
    "    try:\n",
    "        source_vectorization = layers.TextVectorization(\n",
    "            max_tokens=vocab_size,\n",
    "            output_mode=\"int\",\n",
    "            output_sequence_length=sequence_length\n",
    "        )\n",
    "        \n",
    "        #print(f\"lookup 1 {a} {half}\")\n",
    "        source_vectorization.adapt(train_english_texts[a : half])\n",
    "        source_vectorization.get_vocabulary() \n",
    "    except UnicodeDecodeError:\n",
    "        hasError = True\n",
    "        print(f\"the error is between {a} {half}  \")\n",
    "        b = half\n",
    "        continue\n",
    "    \n",
    "    if(a >= b):\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        source_vectorization = layers.TextVectorization(\n",
    "            max_tokens=vocab_size,\n",
    "            output_mode=\"int\",\n",
    "            output_sequence_length=sequence_length\n",
    "        )\n",
    "        #print(f\"lookup 2 {half} {b}\")\n",
    "        source_vectorization.adapt(train_english_texts[half:b])\n",
    "        source_vectorization.get_vocabulary() \n",
    "    except UnicodeDecodeError: \n",
    "        hasError = True\n",
    "        print(f\"the error is between {half} {b}\")\n",
    "        a = half \n",
    "\n",
    "    if(not hasError):\n",
    "        print(\"no enconding error found in the text\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100611\n",
      "['voilà! joined reports and lightning experience together at last.', 'to hide the summary row, select hide summary from the options menu.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'voilà! joined reports and lightning experience together at last.\\tvoilà! verbundene berichte und lightning experience sind endlich vereint.\\n'"
      ]
     },
     "execution_count": 1299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_english_texts))\n",
    "print(train_english_texts[a :b+1 ])\n",
    "pairsArray[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'voilà! joined reports and lightning experience together at last.\\tvoilà! verbundene berichte und lightning experience sind endlich vereint.\\n'"
      ]
     },
     "execution_count": 1300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_english_texts.pop(a)\n",
    "pairsArray.pop(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [],
   "source": [
    "createDatasetFileFromRows(pairsArray, \"train.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tfgpu-clone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9517802f9e486875bd1a90cb9316150f9c80975204a62a359dc8ac0c901cd1ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
