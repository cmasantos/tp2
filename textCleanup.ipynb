{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "FILE_BASE_PATH                              = \"./data/data/ende/\"\n",
    "DEFAULT_OUTPUT_DATASET_FILE_NAME            = \"dataset.txt\"\n",
    "CLEAR_HTML_TAG_REGULAR_EXPRESSION           = re.compile(\"<.*?>\")\n",
    "CLEAR_SPECIAL_HTML_CHARS_REGULAR_EXPRESSION = re.compile(\"(&)\\w+?;\")\n",
    "UKNOWN_CHARS_REGULAR_EXPRESSION             = re.compile(\"[^a-zA-Z0-9 \\t.,-ä)'$’\\\"\\%(#—“”!–öü+éá‘€*ó•\\−―ʼäëïöüçáéíóúñ✦„─]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readJsonFile(path): \n",
    "    file = open(path, encoding=\"utf-8\")\n",
    "\n",
    "    fileAsJson = json.load(file)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "    return fileAsJson\n",
    "\n",
    "def removeHtmlTagsFromString(originalString):\n",
    "    return re.sub(CLEAR_HTML_TAG_REGULAR_EXPRESSION, \"\", originalString)\n",
    "\n",
    "def removeHtmlSpecialCharsFromString(originalString):\n",
    "    return re.sub(CLEAR_SPECIAL_HTML_CHARS_REGULAR_EXPRESSION, \"\", originalString)\n",
    "\n",
    "def removeUnknowChars(originalString): \n",
    "\n",
    "    if(len(re.findall(UKNOWN_CHARS_REGULAR_EXPRESSION, originalString))):\n",
    "        #print(\"removing string: \", originalString)\n",
    "        return \"\"\n",
    "    \n",
    "    return originalString\n",
    "\n",
    "def clearString(originalString):\n",
    "    \n",
    "\n",
    "    x = removeHtmlTagsFromString(originalString)\n",
    "    x = removeHtmlSpecialCharsFromString(x)\n",
    "    x = removeUnknowChars(x)\n",
    "    x = x.replace(\"\\t\", \" \") \n",
    "    x = x.replace(\"voilà!\", \"voila\")\n",
    "    x = x.lower()\n",
    "    return x\n",
    "\n",
    "def readJsonClean(jsonText):\n",
    "    return {key : clearString(value) for key, value in jsonText[\"text\"].items()}\n",
    "\n",
    "def createPairsArray(source, target):\n",
    "    return [f\"{value}\\t{target.get(key)}\\n\" for key, value in source.items()]\n",
    "    \n",
    "def createDatasetFile(source, target, destinationFileName=DEFAULT_OUTPUT_DATASET_FILE_NAME):\n",
    "    outputFile = open(destinationFileName, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    i = 0\n",
    "    for key, value in source.items():\n",
    "        #if(len(target.get(key)) < 3 or len(value) < 3):\n",
    "        #    print(f\"{value}\\t{target.get(key)}\\n\")\n",
    "        outputFile.write(f\"{value}\\t{target.get(key)}\\n\")  \n",
    "\n",
    "    outputFile.close()\n",
    "\n",
    "def createDatasetFileFromRows(rows, destinationFileName=DEFAULT_OUTPUT_DATASET_FILE_NAME):\n",
    "    outputFile = open(destinationFileName, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    i = 0\n",
    "    for row in rows:\n",
    "        #if(len(target.get(key)) < 3 or len(value) < 3):\n",
    "        #    print(f\"{value}\\t{target.get(key)}\\n\")\n",
    "        outputFile.write(row) \n",
    " \n",
    "\n",
    "    outputFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourFilePath   = FILE_BASE_PATH + \"ende_en_dev.json\"\n",
    "targetFilePath = FILE_BASE_PATH + \"ende_de_dev.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jsonSource = readJsonFile(sourFilePath)\n",
    "jsonTarget = readJsonFile(targetFilePath) \n",
    "\n",
    "jsonSourceClean = readJsonClean(jsonSource)\n",
    "jsonTargetClean = readJsonClean(jsonTarget)\n",
    "\n",
    "createDatasetFile(jsonSourceClean, jsonTargetClean)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourFilePath   = FILE_BASE_PATH + \"ende_en_train.json\"\n",
    "targetFilePath = FILE_BASE_PATH + \"ende_de_train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import tensorflow as tf\n",
    "jsonSource = readJsonFile(sourFilePath)\n",
    "jsonTarget = readJsonFile(targetFilePath) \n",
    "\n",
    "jsonSourceClean = readJsonClean(jsonSource)\n",
    "jsonTargetClean = readJsonClean(jsonTarget)  \n",
    "\n",
    "createDatasetFile(jsonSourceClean, jsonTargetClean, \"train.txt\") \n",
    "pairsArray = createPairsArray(jsonSourceClean, jsonTargetClean) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no enconding error found in the text\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import random\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "#text_file = \"./train.txt\"\n",
    "#with open(text_file, encoding='utf-8') as f:\n",
    "#    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "\n",
    "lines = pairsArray\n",
    "\n",
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    english, port = line.split(\"\\t\")\n",
    "    port = \"[start] \" + port + \" [end]\"\n",
    "    text_pairs.append((english, port))\n",
    "\n",
    "train_english_texts = [pair[0] for pair in text_pairs] \n",
    "vocab_size = 15000    # O modelo apneas vai conhecer 15000 palavras\n",
    "sequence_length = 200  # cada frase vai ter 20 palavrasg\n",
    "\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "max = len(train_english_texts)\n",
    "a = 1\n",
    "b = max \n",
    "current = b // 2\n",
    "\n",
    "hasError = False\n",
    "\n",
    "while(( b- a) > 1):\n",
    "    half = a + (b - a) // 2\n",
    "    #print(\"\\ncurrent half: \",half)\n",
    "    try:\n",
    "        source_vectorization = layers.TextVectorization(\n",
    "            max_tokens=vocab_size,\n",
    "            output_mode=\"int\",\n",
    "            output_sequence_length=sequence_length\n",
    "        )\n",
    "        \n",
    "        #print(f\"lookup 1 {a} {half}\")\n",
    "        source_vectorization.adapt(train_english_texts[a : half])\n",
    "        source_vectorization.get_vocabulary() \n",
    "    except UnicodeDecodeError:\n",
    "        hasError = True\n",
    "        print(f\"the error is between {a} {half}  \")\n",
    "        b = half\n",
    "        continue\n",
    "    \n",
    "    if(a >= b):\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        source_vectorization = layers.TextVectorization(\n",
    "            max_tokens=vocab_size,\n",
    "            output_mode=\"int\",\n",
    "            output_sequence_length=sequence_length\n",
    "        )\n",
    "        #print(f\"lookup 2 {half} {b}\")\n",
    "        source_vectorization.adapt(train_english_texts[half:b])\n",
    "        source_vectorization.get_vocabulary() \n",
    "    except UnicodeDecodeError: \n",
    "        hasError = True\n",
    "        print(f\"the error is between {half} {b}\")\n",
    "        a = half \n",
    "\n",
    "    if(not hasError):\n",
    "        print(\"no enconding error found in the text\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100611\n",
      "['voilà! joined reports and lightning experience together at last.', 'to hide the summary row, select hide summary from the options menu.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'voilà! joined reports and lightning experience together at last.\\tvoilà! verbundene berichte und lightning experience sind endlich vereint.\\n'"
      ]
     },
     "execution_count": 1299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_english_texts))\n",
    "print(train_english_texts[a :b+1 ])\n",
    "pairsArray[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'voilà! joined reports and lightning experience together at last.\\tvoilà! verbundene berichte und lightning experience sind endlich vereint.\\n'"
      ]
     },
     "execution_count": 1300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_english_texts.pop(a)\n",
    "pairsArray.pop(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [],
   "source": [
    "createDatasetFileFromRows(pairsArray, \"train.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tfgpu-clone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9517802f9e486875bd1a90cb9316150f9c80975204a62a359dc8ac0c901cd1ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
