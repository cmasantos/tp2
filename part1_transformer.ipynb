{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFydYD8rLnnE"
      },
      "source": [
        "Machine Translation - English to German"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "DPQTZko6LMsF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.9.1\n"
          ]
        }
      ],
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "print(tf.__version__)\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
        "\n",
        "\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "import string\n",
        "import re \n",
        "from keras import layers\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "#input_text_file_path = \"./preprocessed_dataset_for_dev.txt\"\n",
        "input_text_file_path = \"./preprocessed_dataset_for_train.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "xYhzQXWdMJjs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('the bookmarked feed shows the posts that you have bookmarked. the bookmarked feed offers easy access to useful posts.', '[start] im feed mit lesezeichen werden alle post angezeigt, die sie mit einem lesezeichen versehen haben. der feed \"mit lesezeichen\" bietet ihnen schnellen zugriff auf nützliche posts. [end]')\n"
          ]
        }
      ],
      "source": [
        "with open(input_text_file_path, encoding='utf-8') as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "\n",
        "text_pairs = []\n",
        "\n",
        "for line in lines:\n",
        "    english, port = line.split(\"\\t\")\n",
        "    port = \"[start] \" + port + \" [end]\"\n",
        "    text_pairs.append((english, port))\n",
        "\n",
        "print(text_pairs[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Terv66lhM4X5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(text_pairs) -> 100611\n",
            "15% for validation -> 15091\n",
            "70% for train -> 70429\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(text_pairs) #1. mistura todos os pairs\n",
        "\n",
        "print(\"len(text_pairs) ->\", len(text_pairs))\n",
        "\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "print(\"15% for validation ->\", num_val_samples)\n",
        "\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "print(\"70% for train ->\", num_train_samples)\n",
        "\n",
        "train_pairs = text_pairs[:num_train_samples] #escolhe os primeiros 70% (shuffled) para treino\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples] #mais 15% para validação\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:] #mais 15 para teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "X8pce-80NB_u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "treino:  70429\n",
            "validação:  15091\n",
            "teste:  15091\n"
          ]
        }
      ],
      "source": [
        "print(\"treino: \",    len(train_pairs))\n",
        "print(\"validação: \", len(val_pairs  ))\n",
        "print(\"teste: \",     len(test_pairs ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "hX1CnrGxPax4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('12345678912345', '[start] 12345678912345 [end]')\n",
            "12345678912345\n",
            "how often are all my accounts, contacts, leads, and db companies updated with #url1# clean jobs?\n"
          ]
        }
      ],
      "source": [
        "strip_chars = string.punctuation # !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "\n",
        "strip_chars = strip_chars.replace(\"[\", \"\") # para não perder o [start] e [end]\n",
        "strip_chars = strip_chars.replace(\"]\", \"\") #\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 22000    # O modelo apneas vai conhecer 15000 palavras\n",
        "sequence_length = 25  # cada frase vai ter 20 palavrasg\n",
        "\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "print(train_pairs[0])\n",
        "train_english_texts = [pair[0] for pair in train_pairs] \n",
        "print(train_english_texts[0])\n",
        "\n",
        "train_pt_texts = [pair[1] for pair in train_pairs] \n",
        "print(train_english_texts[1])\n",
        "\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "#source_vectorization.adapt([pair[0] for pair in text_pairs] )\n",
        "target_vectorization.adapt(train_pt_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "xY0Eqvx_JRgH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'to', 'a', 'and', 'in', 'you', 'for', 'your']"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "source_vectorization.get_vocabulary()[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "9EfEL9D9Jr2_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', '[start]', '[end]', 'sie', 'die', 'der', 'und', 'in', 'für']"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_vectorization.get_vocabulary()[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EN embeddings has 400000 word vectors\n"
          ]
        }
      ],
      "source": [
        "glove50_file_path = \"./glove.6B.50d.embedding\"\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "with open(glove50_file_path, encoding=\"utf-8\") as gloveFile:\n",
        "    for line in gloveFile:\n",
        "        word, coefsAsString = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefsAsString, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"EN embeddings has {len(embeddings_index)} word vectors\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_matrix size: (22000, 50)\n",
            "[[ 0.68046999 -0.039263    0.30186    -0.17792     0.42962     0.032246\n",
            "  -0.41376001  0.13228001 -0.29846999 -0.085253    0.17117999  0.22419\n",
            "  -0.10046    -0.43652999  0.33418     0.67846     0.057204   -0.34448001\n",
            "  -0.42785001 -0.43274999  0.55962998  0.10032     0.18677001 -0.26853999\n",
            "   0.037334   -2.09319997  0.22171    -0.39868     0.20912001 -0.55725002\n",
            "   3.88260007  0.47466001 -0.95657998 -0.37788001  0.20869    -0.32752001\n",
            "   0.12751     0.088359    0.16350999 -0.21634001 -0.094375    0.018324\n",
            "   0.21048    -0.03088    -0.19722     0.082279   -0.09434    -0.073297\n",
            "  -0.064699   -0.26043999]\n",
            " [ 0.21705     0.46515    -0.46757001  0.10082     1.01349998  0.74844998\n",
            "  -0.53104001 -0.26256001  0.16812     0.13181999 -0.24909    -0.44185001\n",
            "  -0.21739     0.51003999  0.13448    -0.43141001 -0.03123     0.20674001\n",
            "  -0.78138    -0.20148    -0.097401    0.16088    -0.61835998 -0.18504\n",
            "  -0.12461    -2.25259995 -0.22321001  0.5043      0.32257     0.15312999\n",
            "   3.96359992 -0.71364999 -0.67012     0.28388     0.21738     0.14432999\n",
            "   0.25926     0.23434     0.42739999 -0.44451001  0.13812999  0.36973\n",
            "  -0.64288998  0.024142   -0.039315   -0.26036999  0.12017    -0.043782\n",
            "   0.41012999  0.1796    ]]\n"
          ]
        }
      ],
      "source": [
        "en_embeddings_dim = 50\n",
        "\n",
        "en_vocabulary = source_vectorization.get_vocabulary() \n",
        "\n",
        "word_index = dict(zip(en_vocabulary, range(len(en_vocabulary)))) # setting an \"id\" to head word \n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, en_embeddings_dim)) \n",
        "print(\"embedding_matrix size:\", embedding_matrix.shape)\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if(i < vocab_size):\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "\n",
        "    if(embedding_vector is not None):\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(embedding_matrix[3:5])\n",
        "\n",
        "en_embedding_layer = layers.Embedding(vocab_size, en_embeddings_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "yjNBmkmaQE1K"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "def format_dataset(eng, pt):\n",
        "    eng = source_vectorization(eng)\n",
        "    pt = target_vectorization(pt)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"portuguese\": pt[:, :-1],\n",
        "    }, pt[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, pt_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    pt_texts = list(pt_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, pt_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Ptr32zceQLUx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs['english'].shape: (32, 25)\n",
            "inputs['portuguese'].shape: (32, 25)\n",
            "targets.shape: (32, 25)\n",
            "tf.Tensor(\n",
            "[[   17   158  1354     4    61    52   279    41    42     3     2    15\n",
            "     46   128     7    33   313     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [   28   677    62   256   235   126  1995    12   647  1056   312     4\n",
            "    417   108   772   394     8     2   111    56   515     2   164    20\n",
            "   1471]\n",
            " [   40 14818     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [   52    88   202     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [   33   186     6   121  1645     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [   61   254   301   502   237    16   111    46     5   425     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [ 5657  2662   656    43   536   887  5061   970     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [ 2065    32     4  2326    19     4  2482     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [   29     5   183   100   139    10    21     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [  392     3     2   711     5   419    10    82  1097   553    12   368\n",
            "    272   249  2189    92  1032     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [   26     7   505     4    24     2   332  1693     4   262    43 14417\n",
            "     35  6201     2    57    43     5  1333  7589   358     8    65   307\n",
            "   8544]\n",
            " [  236   342     5     2   236   142    34    71   122     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [   28    13     4   135    22    12  1085     2   210    50     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [ 3818     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [  128     4  2810    13   536   260    94   922     3    54    53     3\n",
            "     40     5   260    94    20    53    50     3   794     5  2060   182\n",
            "     10]\n",
            " [   40    46   934 10269     6     2    36    86  1176  1156     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [ 1886    10    57    89  2073    35   344    59   535   223    11   136\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [14570     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [    8   101    77    60  1075   222     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [  849   752     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [   28   281   256     2   192   933   358    10     4    43     5     2\n",
            "    295  1544   358    10     4   467   252    78  2947    35     4  3142\n",
            "    689]\n",
            " [    3    29     4    24   122    33    61     5   496     2  1961     3\n",
            "     63    23   244   122    33    63     5   152     9   226     0     0\n",
            "      0]\n",
            " [  467   450     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [    2   176   171   672   418     3   135    56    46  1071     3     2\n",
            "    129   167    22   812     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [  156   720    52  2588    38    68  1046     6    15   124     7    99\n",
            "    103   106    76   157     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [  943     2   642   580     3     4   178     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [16130     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [  469     2   773     4    43     5   177    43     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [   36   408   477    90     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]\n",
            " [   28   198    13    58    53    17     2    31   807   177   198    13\n",
            "    165     6    12    76     2  1161  1328  1339     8     2   809    17\n",
            "      4]\n",
            " [  388     2   267    81     8     2    41  1207   277     3    60    17\n",
            "      2  1269  1207  1673    17     2  1207   224  1278    29    42     0\n",
            "      0]\n",
            " [   29   586     8     9   223    21    66     2   709   754    37    59\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0]], shape=(32, 25), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['portuguese'].shape: {inputs['portuguese'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")\n",
        "    print(inputs['english'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7r7M3lbPTCYx"
      },
      "outputs": [],
      "source": [
        "# Criação da classe que modela o Encoder \n",
        "\n",
        "# Na criação do objeto recebe \n",
        "# embed_dim: Dimensão da sequência de input \n",
        "# dense_dim: Número de nós da camada Dense\n",
        "# num_heads: Número de attention heads\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        self.dense_dim = dense_dim\n",
        "        \n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
        "        \n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        \n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        \n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        \n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        \n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        \n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        \n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        \n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "1jFFDfMoQdER"
      },
      "outputs": [],
      "source": [
        "# Criação da classe que modela o Decoder \n",
        "\n",
        "# Na criação do objeto recebe \n",
        "# embed_dim: Dimensão da sequência de input \n",
        "# dense_dim: Número de nós da camada Dense\n",
        "# num_heads: Número de attention heads\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        \n",
        "        super().__init__(**kwargs)\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        self.dense_dim = dense_dim\n",
        "        \n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        \n",
        "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "\n",
        "        self.dense_proj = keras.Sequential([layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
        "        \n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        \n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        \n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        \n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        \n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        \n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        \n",
        "        input_shape = tf.shape(inputs)\n",
        "        \n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        \n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        \n",
        "        j = tf.range(sequence_length)\n",
        "        \n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        \n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        \n",
        "        mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        \n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        \n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        \n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        \n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        \n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        \n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask)\n",
        "        \n",
        "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        \n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "4jfXOf3nTMOl"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, wordEmbedding=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        if( wordEmbedding is not None ):\n",
        "            self.token_embeddings = wordEmbedding\n",
        "        else:\n",
        "            self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
        "        \n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        \n",
        "        self.sequence_length = sequence_length\n",
        "        \n",
        "        self.input_dim = input_dim\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \n",
        "        length = tf.shape(inputs)[-1]\n",
        "        \n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        \n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        \n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        \n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "JW_VGzs1TPcZ"
      },
      "outputs": [],
      "source": [
        "# The complete Transformer\n",
        "\n",
        "import keras\n",
        "\n",
        "# Settings \n",
        "\n",
        "#embed_dim = 256\n",
        "embed_dim = 50\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim, en_embedding_layer)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"portuguese\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "best_model_file_path = \"best_translator.tfmodel\"\n",
        "callbacks_list = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy', \n",
        "        patience = 10, \n",
        "        min_delta = 0.005, \n",
        "        restore_best_weights = True,\n",
        "        mode='auto'\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath = best_model_file_path,\n",
        "        monitor = \"val_accuracy\",\n",
        "        verbose=0,\n",
        "        save_best_only = True,\n",
        "        save_weights_only = False,\n",
        "        mode='max'\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "FdhUT_gqTX8w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 3.1111 - accuracy: 0.2158"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 77s 34ms/step - loss: 3.1110 - accuracy: 0.2159 - val_loss: 2.7025 - val_accuracy: 0.2822\n",
            "Epoch 2/25\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 2.8099 - accuracy: 0.2834"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 73s 33ms/step - loss: 2.8099 - accuracy: 0.2834 - val_loss: 2.5430 - val_accuracy: 0.3236\n",
            "Epoch 3/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.6514 - accuracy: 0.3182"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 73s 33ms/step - loss: 2.6513 - accuracy: 0.3182 - val_loss: 2.4036 - val_accuracy: 0.3563\n",
            "Epoch 4/25\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 2.5234 - accuracy: 0.3459"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 74s 33ms/step - loss: 2.5234 - accuracy: 0.3459 - val_loss: 2.3289 - val_accuracy: 0.3792\n",
            "Epoch 5/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.5130 - accuracy: 0.3660"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 74s 34ms/step - loss: 2.5129 - accuracy: 0.3660 - val_loss: 2.3699 - val_accuracy: 0.3928\n",
            "Epoch 6/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.5137 - accuracy: 0.3826"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 74s 33ms/step - loss: 2.5136 - accuracy: 0.3826 - val_loss: 2.3674 - val_accuracy: 0.4026\n",
            "Epoch 7/25\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 2.5005 - accuracy: 0.3957"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 74s 34ms/step - loss: 2.5005 - accuracy: 0.3957 - val_loss: 2.3620 - val_accuracy: 0.4100\n",
            "Epoch 8/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.4862 - accuracy: 0.4061"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 74s 34ms/step - loss: 2.4861 - accuracy: 0.4062 - val_loss: 2.3546 - val_accuracy: 0.4167\n",
            "Epoch 9/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.4699 - accuracy: 0.4154"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 73s 33ms/step - loss: 2.4698 - accuracy: 0.4155 - val_loss: 2.3507 - val_accuracy: 0.4222\n",
            "Epoch 10/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.4520 - accuracy: 0.4244"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 73s 33ms/step - loss: 2.4519 - accuracy: 0.4245 - val_loss: 2.3430 - val_accuracy: 0.4265\n",
            "Epoch 11/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.4343 - accuracy: 0.4312"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 74s 34ms/step - loss: 2.4342 - accuracy: 0.4312 - val_loss: 2.3378 - val_accuracy: 0.4307\n",
            "Epoch 12/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.4168 - accuracy: 0.4379"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 75s 34ms/step - loss: 2.4168 - accuracy: 0.4380 - val_loss: 2.3301 - val_accuracy: 0.4332\n",
            "Epoch 13/25\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 2.4002 - accuracy: 0.4440"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 74s 34ms/step - loss: 2.4002 - accuracy: 0.4440 - val_loss: 2.3281 - val_accuracy: 0.4373\n",
            "Epoch 14/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.3841 - accuracy: 0.4496"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 74s 34ms/step - loss: 2.3841 - accuracy: 0.4496 - val_loss: 2.3223 - val_accuracy: 0.4401\n",
            "Epoch 15/25\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 2.3679 - accuracy: 0.4547"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 74s 34ms/step - loss: 2.3679 - accuracy: 0.4547 - val_loss: 2.3170 - val_accuracy: 0.4413\n",
            "Epoch 16/25\n",
            "2201/2201 [==============================] - 70s 32ms/step - loss: 2.3529 - accuracy: 0.4592 - val_loss: 2.3240 - val_accuracy: 0.4406\n",
            "Epoch 17/25\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 2.3403 - accuracy: 0.4635"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 76s 34ms/step - loss: 2.3403 - accuracy: 0.4635 - val_loss: 2.3178 - val_accuracy: 0.4414\n",
            "Epoch 18/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.3265 - accuracy: 0.4675"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 76s 34ms/step - loss: 2.3264 - accuracy: 0.4675 - val_loss: 2.3179 - val_accuracy: 0.4434\n",
            "Epoch 19/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.3139 - accuracy: 0.4711"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 76s 34ms/step - loss: 2.3138 - accuracy: 0.4711 - val_loss: 2.3094 - val_accuracy: 0.4460\n",
            "Epoch 20/25\n",
            "2201/2201 [==============================] - 71s 32ms/step - loss: 2.3027 - accuracy: 0.4744 - val_loss: 2.3142 - val_accuracy: 0.4451\n",
            "Epoch 21/25\n",
            "2200/2201 [============================>.] - ETA: 0s - loss: 2.2909 - accuracy: 0.4780"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 78s 35ms/step - loss: 2.2909 - accuracy: 0.4780 - val_loss: 2.3077 - val_accuracy: 0.4481\n",
            "Epoch 22/25\n",
            "2201/2201 [==============================] - 73s 33ms/step - loss: 2.2813 - accuracy: 0.4807 - val_loss: 2.3138 - val_accuracy: 0.4466\n",
            "Epoch 23/25\n",
            "2201/2201 [==============================] - 70s 32ms/step - loss: 2.2701 - accuracy: 0.4840 - val_loss: 2.3176 - val_accuracy: 0.4470\n",
            "Epoch 24/25\n",
            "2201/2201 [==============================] - 70s 32ms/step - loss: 2.2602 - accuracy: 0.4867 - val_loss: 2.3182 - val_accuracy: 0.4462\n",
            "Epoch 25/25\n",
            "2201/2201 [==============================] - ETA: 0s - loss: 2.2506 - accuracy: 0.4892"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_8_layer_call_fn, embedding_8_layer_call_and_return_conditional_losses, embedding_9_layer_call_fn, embedding_9_layer_call_and_return_conditional_losses, embedding_10_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: best_translator.tfmodel\\assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2201/2201 [==============================] - 74s 34ms/step - loss: 2.2506 - accuracy: 0.4892 - val_loss: 2.3145 - val_accuracy: 0.4482\n"
          ]
        }
      ],
      "source": [
        "history = transformer.fit(\n",
        "    train_ds,\n",
        "    epochs=25,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "_h6nkceEYpUH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-\n",
            "article versions\n",
            "[start] [UNK] [end]\n",
            "-\n",
            "from the filters tab, under cross filters, find the cross filter you want to edit.\n",
            "[start] wechseln sie auf der registerkarte [UNK] [UNK] [end]\n",
            "-\n",
            "for your idp issuer / entity id, enter your salesforce identity provider issuer, for example, #url1#.\n",
            "[start] geben sie für ihre [UNK] ein geben sie ihre [UNK] für den [UNK] ein beispielsweise url1 [end]\n",
            "-\n",
            "step 2: restrict oauth connected app access (whitelist apps)\n",
            "[start] datum 1 [UNK] für verbundene anwendung zugriff auf anwendungen [end]\n",
            "-\n",
            "edit from streams home\n",
            "[start] bearbeiten auf der seite [end]\n",
            "-\n",
            "to add compose gmail buttons in activity history on leads and contacts, select gmail buttons.\n",
            "[start] hinzufügen von [UNK] zu [UNK] in leads und benutzerdefinierten [UNK] [end]\n",
            "-\n",
            "contact support button\n",
            "[start] [UNK] [end]\n",
            "-\n",
            "based on these matching criteria, here's how matching works.\n",
            "[start] im folgenden finden sie die folgenden [UNK] beispielsweise [UNK] [end]\n",
            "-\n",
            "conflict behavior—salesforce always wins\n",
            "[start] wenn die [UNK] für die [UNK] [UNK] ist [end]\n",
            "-\n",
            "you can only select profiles that are associated with the community. if a profile is selected as the default for users who self-register and you remove the profile from the community, the profile resets to none.\n",
            "[start] sie können nur auswählen dass die mit der [UNK] zugeordnet sind wenn ein profil als community aktiviert ist wird der benutzer für die [UNK] und\n",
            "-\n",
            "salesforce sites enables you to create public websites and applications that are directly integrated with your salesforce organization—without requiring users to log in with a username and password.\n",
            "[start] [UNK] mit der sie [UNK] für anwendungen die sie erstellen und anwendungen die mit ihrem [UNK] in der [UNK] [UNK] in einem [UNK] [UNK] sind\n",
            "-\n",
            "in the third field, enter the filter value.\n",
            "[start] geben sie im feld [UNK] den wert für den [UNK] ein [end]\n",
            "-\n",
            "target element—specifies the page element that the action affects\n",
            "[start] die [UNK] die [UNK] die das [UNK] ist [end]\n",
            "-\n",
            "c - top 10 files by shares\n",
            "[start] status [UNK] und [UNK] nach [UNK] [end]\n",
            "-\n",
            "chatter groups with customers don't support global create, log a call, or custom actions and display only standard chatter actions, such as post, file, link, and poll.\n",
            "[start] [UNK] sind mit kunden nicht unterstützt die [UNK] oder einen [UNK] für benutzerdefinierte aktionen und [UNK] sind auch auf [UNK] angezeigt beispielsweise [UNK] [UNK] [UNK]\n",
            "-\n",
            "you can't apply a milestone to a record by itself. it must be part of an entitlement process. so after you create your milestone, add it to an entitlement process.\n",
            "[start] sie können nicht auf einen datensatz festlegen um einen datensatz zu können sie die [UNK] für einen [UNK] festlegen wenn sie den [UNK] erstellen möchten\n",
            "-\n",
            "add the entitlement name field to case and work order page layouts assigned to community users. this lets users add entitlements to cases and work orders. work orders are not available in communities built using the self-service community templates.\n",
            "[start] fügen sie den namen des [UNK] für kundenvorgänge und [UNK] hinzu die benutzer diese benutzer können diese benutzer mit den [UNK] hinzufügen von benutzern und\n",
            "-\n",
            "specify each member's role on the opportunity, such as executive sponsor.\n",
            "[start] geben sie die option für die opportunity für jedes [UNK] [UNK] an [end]\n",
            "-\n",
            "save for a salesforce console\n",
            "[start] speichern einer salesforce classic [end]\n",
            "-\n",
            "enter the text that directs users to a login page before they ask a question. the default is: sign in to ask a question.\n",
            "[start] geben sie den text ein der benutzer zu einer seite [UNK] stellen sie sicher dass die fragen in der community für die frage an die\n"
          ]
        }
      ],
      "source": [
        "# Testar o desempenho do Transformer em frases do conjunto de teste\n",
        "\n",
        "\n",
        "pt_vocab = target_vectorization.get_vocabulary()\n",
        "pt_index_lookup = dict(zip(range(len(pt_vocab)), pt_vocab))\n",
        "max_decoded_sentence_length = 25\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "   \n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "   \n",
        "    decoded_sentence = \"[start]\"\n",
        "   \n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "        \n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        \n",
        "        sampled_token = pt_index_lookup[sampled_token_index]\n",
        "        \n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        \n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    \n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "UONQEruoZIyh"
      },
      "outputs": [],
      "source": [
        "#for _ in range(5):\n",
        "#    input_sentence = input()\n",
        "#    print(\"-\")\n",
        "#    print(input_sentence)\n",
        "#    print(decode_sequence(input_sentence), '\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "AD2122_Aula9_Transformer_ENG-PT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('tfgpu-clone')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "9517802f9e486875bd1a90cb9316150f9c80975204a62a359dc8ac0c901cd1ea"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
